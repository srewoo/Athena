import { useState, useEffect } from 'react';
import axios from 'axios';
import { Button } from '../ui/button';
import { Input } from '../ui/input';
import { Label } from '../ui/label';
import { Textarea } from '../ui/textarea';
import { Card, CardContent, CardDescription, CardHeader, CardTitle } from '../ui/card';
import { Badge } from '../ui/badge';
import { Dialog, DialogContent, DialogDescription, DialogHeader, DialogTitle } from '../ui/dialog';
import { toast } from 'sonner';
import { Sparkles, Star, RefreshCw, Loader2, Wand2, CheckCircle2, Info, Download, ShieldCheck, AlertTriangle } from 'lucide-react';
// import SuiteAnalysis from '../suite-analysis/SuiteAnalysis'; // Temporarily disabled due to Babel plugin issue
import EvalFeedback from '../eval-feedback/EvalFeedback';

const BACKEND_URL = process.env.REACT_APP_BACKEND_URL;
const API = `${BACKEND_URL}/api`;

export default function EvalGeneration({ project, selectedVersion, settings }) {
  const [dimension, setDimension] = useState('');
  const [systemPrompt, setSystemPrompt] = useState('');
  const [generatedEval, setGeneratedEval] = useState(null);
  const [loading, setLoading] = useState(false);
  const [evalPrompts, setEvalPrompts] = useState([]);
  const [generatingDimensions, setGeneratingDimensions] = useState(false);
  const [dimensions, setDimensions] = useState([]);
  const [dimensionsText, setDimensionsText] = useState('');
  const [showDimensionsModal, setShowDimensionsModal] = useState(false);
  const [dimensionAnalysis, setDimensionAnalysis] = useState(null);
  const [hasAutoGenerated, setHasAutoGenerated] = useState(false);
  const [selectedEvalPrompt, setSelectedEvalPrompt] = useState(null);
  const [showEvalPromptModal, setShowEvalPromptModal] = useState(false);
  const [metaEvaluations, setMetaEvaluations] = useState({});
  const [showMetaEvalModal, setShowMetaEvalModal] = useState(false);
  const [selectedMetaEval, setSelectedMetaEval] = useState(null);
  const [metaEvaluating, setMetaEvaluating] = useState(false);
  const [addingToMaxim, setAddingToMaxim] = useState(false);

  // Validation state (Feature 2 & 3)
  const [validationResults, setValidationResults] = useState({});
  const [validating, setValidating] = useState({});
  const [refining, setRefining] = useState(false);

  useEffect(() => {
    if (project) {
      loadEvalPrompts();
    }
    if (selectedVersion) {
      setSystemPrompt(selectedVersion.content);
    } else {
      loadLatestVersion();
    }
  }, [project, selectedVersion]);

  // Auto-generate dimensions when component loads (first time only)
  useEffect(() => {
    const shouldAutoGenerate =
      systemPrompt &&
      !hasAutoGenerated &&
      !generatingDimensions &&
      dimensions.length === 0 &&
      settings?.openai_key;

    if (shouldAutoGenerate) {
      setHasAutoGenerated(true);
      toast.info('ðŸ” Auto-generating evaluation dimensions...', {
        duration: 2000
      });
      setTimeout(() => {
        handleGenerateDimensions(true);
      }, 500);
    }
  }, [systemPrompt, hasAutoGenerated, generatingDimensions, dimensions.length, settings]);

  const loadLatestVersion = async () => {
    if (!project) return;
    try {
      const response = await axios.get(`${API}/prompt-versions/${project.id}`);
      if (response.data.length > 0) {
        setSystemPrompt(response.data[0].content);
      }
    } catch (error) {
      console.error('Error loading prompt versions:', error);
    }
  };

  const loadEvalPrompts = async () => {
    try {
      const response = await axios.get(`${API}/eval-prompts/${project.id}`);
      setEvalPrompts(response.data);
    } catch (error) {
      console.error('Error loading eval prompts:', error);
    }
  };

  const handleAddToMaxim = async (evalPromptId) => {
    if (!settings?.maxim_api_key || !settings?.maxim_workspace_id) {
      toast.error('Maxim not configured', {
        description: 'Please add Maxim API Key and Workspace ID in Settings'
      });
      return;
    }

    setAddingToMaxim(true);
    try {
      const response = await axios.post(
        `${API}/eval-prompts/${evalPromptId}/add-to-maxim?session_id=${settings.session_id}`
      );

      if (response.data.success) {
        toast.success('Added to Maxim!', {
          description: `Evaluator "${response.data.evaluator_name}" created successfully`
        });
      } else {
        toast.error('Failed to add to Maxim', {
          description: response.data.error || 'Unknown error'
        });
      }
    } catch (error) {
      console.error('Error adding to Maxim:', error);
      toast.error('Failed to add to Maxim', {
        description: error.response?.data?.detail || error.message
      });
    } finally {
      setAddingToMaxim(false);
    }
  };

  const handleGenerateDimensions = async (isAutoTriggered = false) => {
    if (!systemPrompt || !settings?.openai_key) {
      if (!isAutoTriggered) {
        toast.error('Please configure API keys in Settings');
      }
      return;
    }

    setGeneratingDimensions(true);
    try {
      const provider = settings.default_provider || 'openai';
      let apiKey;
      if (provider === 'openai') {
        apiKey = settings.openai_key;
      } else if (provider === 'anthropic') {
        apiKey = settings.claude_key;
      } else if (provider === 'google') {
        apiKey = settings.gemini_key;
      }

      const response = await axios.post(`${API}/generate-eval-dimensions`, {
        prompt_content: systemPrompt,
        provider: provider,
        model: settings.generation_model || settings.default_model || 'gpt-4o',
        api_key: apiKey,
      });

      if (response.data.success) {
        setDimensionAnalysis(response.data);
        const generatedDimensions = response.data.dimensions || [];
        setDimensions(generatedDimensions);
        
        // Convert dimensions to text format with descriptions (one per line)
        const dimensionsTextContent = generatedDimensions
          .map(dim => `${dim.name}: ${dim.description || ''}`)
          .join('\n\n');
        setDimensionsText(dimensionsTextContent);
        
        if (isAutoTriggered) {
          toast.success(`âœ¨ Generated ${generatedDimensions.length} evaluation dimensions!`, {
            description: `Complexity: ${response.data.complexity_level} - You can edit them below`,
            duration: 4000
          });
        } else {
          toast.success(`âœ¨ Generated ${generatedDimensions.length} evaluation dimensions!`, {
            description: `Complexity: ${response.data.complexity_level}`,
            duration: 4000
          });
        }
      } else {
        toast.error('Failed to generate dimensions');
      }
    } catch (error) {
      console.error('Error generating dimensions:', error);
      if (!isAutoTriggered) {
        toast.error('Dimension generation failed');
      }
    } finally {
      setGeneratingDimensions(false);
    }
  };

  const handleAddMoreDimensions = async () => {
    if (!systemPrompt || !settings?.openai_key) {
      toast.error('Please configure API keys in Settings');
      return;
    }

    setGeneratingDimensions(true);
    try {
      const provider = settings.default_provider || 'openai';
      let apiKey;
      if (provider === 'openai') {
        apiKey = settings.openai_key;
      } else if (provider === 'anthropic') {
        apiKey = settings.claude_key;
      } else if (provider === 'google') {
        apiKey = settings.gemini_key;
      }

      // Extract existing dimension names to avoid overlap
      const existingDimensionNames = dimensions.map(d => d.name);

      const response = await axios.post(`${API}/generate-eval-dimensions`, {
        prompt_content: systemPrompt,
        provider: provider,
        model: settings.generation_model || settings.default_model || 'gpt-4o',
        api_key: apiKey,
        existing_dimensions: existingDimensionNames, // Pass existing to avoid overlap
      });

      if (response.data.success) {
        const newDimensions = response.data.dimensions || [];

        // Filter out any dimensions that still overlap (double-check)
        const uniqueNewDimensions = newDimensions.filter(
          newDim => !existingDimensionNames.includes(newDim.name)
        );

        if (uniqueNewDimensions.length === 0) {
          toast.info('No new unique dimensions generated. All suggestions overlapped with existing ones.', {
            duration: 4000
          });
          return;
        }

        // Append new dimensions to existing ones
        const allDimensions = [...dimensions, ...uniqueNewDimensions];
        setDimensions(allDimensions);

        // Append to existing text (add separator)
        const newDimensionsText = uniqueNewDimensions
          .map(dim => `${dim.name}: ${dim.description || ''}`)
          .join('\n\n');

        const updatedText = dimensionsText
          ? `${dimensionsText}\n\n${newDimensionsText}`
          : newDimensionsText;

        setDimensionsText(updatedText);

        toast.success(`âœ¨ Added ${uniqueNewDimensions.length} new unique dimensions!`, {
          description: `Total: ${allDimensions.length} dimensions (no overlap)`,
          duration: 4000
        });
      } else {
        toast.error('Failed to generate more dimensions');
      }
    } catch (error) {
      console.error('Error adding more dimensions:', error);
      toast.error('Failed to add more dimensions');
    } finally {
      setGeneratingDimensions(false);
    }
  };

  const handleClearAllEvals = async () => {
    if (!project?.id) return;

    try {
      await axios.delete(`${API}/eval-prompts/${project.id}`);
      setEvalPrompts([]);
      setMetaEvaluations({});
      toast.success('All evaluation prompts cleared!');
    } catch (error) {
      console.error('Error clearing eval prompts:', error);
      toast.error('Failed to clear evaluation prompts');
    }
  };

  const getApiKey = () => {
    const provider = settings?.default_provider || 'openai';
    if (provider === 'openai') return settings?.openai_key;
    if (provider === 'anthropic') return settings?.claude_key;
    if (provider === 'google') return settings?.gemini_key;
    return null;
  };

  const handleValidateEval = async (evalPrompt, e) => {
    if (e) e.stopPropagation();
    const apiKey = getApiKey();
    const provider = settings?.default_provider || 'openai';
    if (!apiKey) { toast.error('API key required'); return; }

    setValidating(prev => ({ ...prev, [evalPrompt.id]: true }));
    try {
      const response = await axios.post(`${API}/validate-eval-prompt`, {
        eval_prompt_id: evalPrompt.id,
        project_id: project.id,
        system_prompt: systemPrompt,
        eval_prompt_content: evalPrompt.content,
        dimension: evalPrompt.dimension,
        provider: provider,
        model: settings?.generation_model || settings?.default_model || 'gpt-4o',
        api_key: apiKey,
        session_id: settings?.session_id,
      });
      setValidationResults(prev => ({ ...prev, [evalPrompt.id]: response.data }));
      const overall = response.data.overall_quality;
      if (overall === 'strong') {
        toast.success(`"${evalPrompt.dimension}" validation: Strong`, { description: 'Both discrimination and consistency are good' });
      } else if (overall === 'weak') {
        toast.warning(`"${evalPrompt.dimension}" validation: Weak`, { description: 'Consider refining this eval prompt' });
      } else {
        toast.info(`"${evalPrompt.dimension}" validation: Moderate`, { description: 'Some aspects could be improved' });
      }
    } catch (error) {
      toast.error(`Validation failed: ${error.response?.data?.detail || error.message}`);
    } finally {
      setValidating(prev => ({ ...prev, [evalPrompt.id]: false }));
    }
  };

  const handleRefineWithEvidence = async (evalPrompt) => {
    const apiKey = getApiKey();
    const provider = settings?.default_provider || 'openai';
    if (!apiKey) { toast.error('API key required'); return; }

    const validation = validationResults[evalPrompt.id];
    if (!validation) { toast.error('Run validation first'); return; }

    setRefining(true);
    try {
      const response = await axios.post(`${API}/refine-eval-with-evidence`, {
        eval_prompt_id: evalPrompt.id,
        project_id: project.id,
        prompt_version_id: selectedVersion?.id || 'latest',
        system_prompt: systemPrompt,
        current_eval_content: evalPrompt.content,
        dimension: evalPrompt.dimension,
        validation_result: validation,
        provider: provider,
        model: settings?.generation_model || settings?.default_model || 'gpt-4o',
        api_key: apiKey,
        session_id: settings?.session_id,
      });

      // Update eval in local state
      setEvalPrompts(prev => prev.map(ep =>
        ep.id === evalPrompt.id ? response.data : ep
      ));
      setSelectedEvalPrompt(response.data);

      // Clear old validation
      setValidationResults(prev => {
        const next = { ...prev };
        delete next[evalPrompt.id];
        return next;
      });

      toast.success(`Eval refined! New quality: ${response.data.quality_score?.toFixed(1)}/10`, {
        description: 'Run "Validate" again to verify improvement'
      });
    } catch (error) {
      toast.error(`Refinement failed: ${error.response?.data?.detail || error.message}`);
    } finally {
      setRefining(false);
    }
  };

  const getValidationBadgeColor = (rating) => {
    if (rating === 'good') return 'bg-green-600 text-white';
    if (rating === 'fair') return 'bg-yellow-500 text-white';
    return 'bg-red-600 text-white';
  };

  const handleDownloadCSV = () => {
    if (evalPrompts.length === 0) {
      toast.error('No prompts to download');
      return;
    }

    try {
      // Create CSV header
      const headers = ['Dimension', 'Quality Score', 'Prompt Content', 'Created At'];

      // Create CSV rows
      const rows = evalPrompts.map(ep => {
        return [
          ep.dimension || '',
          ep.quality_score?.toFixed(1) || 'N/A',
          // Escape quotes in prompt content and wrap in quotes
          `"${(ep.content || '').replace(/"/g, '""')}"`,
          ep.created_at ? new Date(ep.created_at).toLocaleString() : ''
        ];
      });

      // Combine header and rows
      const csvContent = [
        headers.join(','),
        ...rows.map(row => row.join(','))
      ].join('\n');

      // Create blob and download
      const blob = new Blob([csvContent], { type: 'text/csv;charset=utf-8;' });
      const link = document.createElement('a');
      const url = URL.createObjectURL(blob);

      link.setAttribute('href', url);
      link.setAttribute('download', `eval-prompts-${project?.name || 'export'}-${new Date().toISOString().split('T')[0]}.csv`);
      link.style.visibility = 'hidden';

      document.body.appendChild(link);
      link.click();
      document.body.removeChild(link);

      toast.success(`Downloaded ${evalPrompts.length} prompt${evalPrompts.length !== 1 ? 's' : ''} as CSV`);
    } catch (error) {
      console.error('Error downloading CSV:', error);
      toast.error('Failed to download CSV');
    }
  };

  const handleGenerateAllEvals = async (clearFirst = false) => {
    if (!systemPrompt || !settings?.openai_key) {
      toast.error('Please configure API keys in Settings');
      return;
    }

    if (!dimensionsText || dimensionsText.trim().length === 0) {
      toast.error('Please generate or enter evaluation dimensions first');
      return;
    }

    // Parse dimensions from textarea (format: "name: description" or just "name")
    const dimensionsList = dimensionsText
      .split('\n')
      .map(line => line.trim())
      .filter(line => line.length > 0)
      .map(line => {
        // Remove bullet points if present
        line = line.replace(/^[-*â€¢]\s*/, '');
        // Extract dimension name and description (split by colon)
        const colonIndex = line.indexOf(':');
        if (colonIndex > 0) {
          return {
            name: line.substring(0, colonIndex).trim(),
            description: line.substring(colonIndex + 1).trim()
          };
        } else {
          return {
            name: line.trim(),
            description: null
          };
        }
      });

    if (dimensionsList.length === 0) {
      toast.error('No valid dimensions found');
      return;
    }

    // Clear existing eval prompts if requested
    if (clearFirst) {
      await handleClearAllEvals();
    }

    // Get correct API key based on provider
    const provider = settings.default_provider || 'openai';
    let apiKey;
    if (provider === 'openai') {
      apiKey = settings.openai_key;
    } else if (provider === 'anthropic') {
      apiKey = settings.claude_key;
    } else if (provider === 'google') {
      apiKey = settings.gemini_key;
    }

    toast.info(`ðŸš€ Generating ${dimensionsList.length} evaluation prompts in parallel...`, {
      description: settings?.domain_context
        ? 'Using domain context for domain-specific prompts'
        : 'This will be much faster!',
      duration: 3000
    });

    setLoading(true);

    try {
      // Generate eval prompts in parallel using Promise.all
      const generatePromises = dimensionsList.map(dim =>
        axios.post(`${API}/eval-prompts`, {
          project_id: project.id,
          prompt_version_id: selectedVersion?.id || 'latest',
          system_prompt: systemPrompt,
          dimension: dim.name,
          dimension_description: dim.description,  // ðŸŽ¯ NEW: Pass dimension description for better threshold detection
          provider: provider,
          model: settings.generation_model || settings.default_model || 'gpt-4o',
          api_key: apiKey,
          session_id: settings?.session_id,  // Include session_id for domain context
        })
        .then(response => ({ success: true, dimension: dim.name, data: response.data }))
        .catch(error => {
          console.error(`Error generating eval for ${dim.name}:`, error);
          return { success: false, dimension: dim.name, error };
        })
      );

      // Wait for all requests to complete
      const results = await Promise.all(generatePromises);
      
      // Count successes and failures
      const successCount = results.filter(r => r.success).length;
      const failCount = results.filter(r => !r.success).length;
      
      // Reload the eval prompts list
      await loadEvalPrompts();
      
      if (successCount > 0) {
        toast.success(`âœ… Generated ${successCount} evaluation prompts in parallel!`, {
          description: failCount > 0 
            ? `${failCount} failed - check console for details` 
            : `All ${successCount} prompts generated successfully`,
          duration: 5000
        });

        // Perform meta-evaluation on successfully generated prompts
        toast.info('ðŸ” Running quality checks on generated prompts...', {
          duration: 3000
        });
        
        setMetaEvaluating(true);
        
        const metaEvalPromises = results
          .filter(r => r.success)
          .map(r => 
            axios.post(`${API}/meta-evaluate`, {
              system_prompt: systemPrompt,
              eval_prompt: r.data.content,
              eval_prompt_id: r.data.id,
              dimension: r.data.dimension,
              provider: settings.default_provider || 'openai',
              model: 'gpt-4o', // Use higher quality model for meta-eval
              api_key: settings.openai_key || settings.claude_key || settings.gemini_key,
            })
            .then(response => ({ 
              success: true, 
              evalPromptId: r.data.id, 
              dimension: r.data.dimension,
              analysis: response.data.analysis 
            }))
            .catch(error => {
              console.error(`Meta-eval error for ${r.data.dimension}:`, error);
              return { success: false, evalPromptId: r.data.id, dimension: r.data.dimension };
            })
          );

        const metaResults = await Promise.all(metaEvalPromises);
        
        // Store meta-evaluations in state
        const metaEvalMap = {};
        metaResults.forEach(result => {
          if (result.success) {
            metaEvalMap[result.evalPromptId] = {
              dimension: result.dimension,
              analysis: result.analysis
            };
          }
        });
        setMetaEvaluations(prev => ({ ...prev, ...metaEvalMap }));
        
        setMetaEvaluating(false);
        
        const metaSuccessCount = metaResults.filter(r => r.success).length;
        if (metaSuccessCount > 0) {
          toast.success(`âœ… Quality checks completed for ${metaSuccessCount} prompts!`, {
            description: 'Click on any prompt to view feedback',
            duration: 5000
          });
        }
      } else {
        toast.error('All evaluation prompt generations failed', {
          description: 'Check console for error details'
        });
      }
    } catch (error) {
      console.error('Error in parallel generation:', error);
      toast.error('Generation failed');
    } finally {
      setLoading(false);
    }
  };

  if (!project) {
    return (
      <div className="text-center py-12">
        <p className="text-muted-foreground">Please create a project first</p>
      </div>
    );
  }

  return (
    <div className="space-y-6 pb-24">
      <div>
        <h2 className="text-3xl font-bold mb-2" data-testid="eval-generation-title">Step 3: Evaluation Prompt Generation</h2>
        <p className="text-muted-foreground">
          AI-powered generation with quality assurance pipeline
        </p>
      </div>

      <div className="grid grid-cols-1 lg:grid-cols-2 gap-6">
        {/* Generation Config */}
        <Card>
          <CardHeader>
            <CardTitle>Generate Evaluation Prompt</CardTitle>
            <CardDescription>
              Auto-analyzes system prompt, extracts context, and retains input variables
              {settings?.domain_context && (
                <div className="mt-2 flex items-center space-x-2 text-green-600 dark:text-green-400">
                  <CheckCircle2 className="w-4 h-4" />
                  <span className="text-xs font-medium">
                    Domain context active - generating domain-specific prompts
                  </span>
                </div>
              )}
            </CardDescription>
          </CardHeader>
          <CardContent className="space-y-4">
            <div className="space-y-2">
              <Label>System Prompt Context</Label>
              <Textarea
                value={systemPrompt}
                onChange={(e) => setSystemPrompt(e.target.value)}
                rows={8}
                className="font-mono text-sm"
                data-testid="eval-system-prompt"
              />
              <p className="text-xs text-muted-foreground">
                Variables like {'{{'}}input{'}}'}, {'{{'}}query{'}}'} will be auto-detected
              </p>
            </div>

            <div className="space-y-2">
              <div className="flex items-center justify-between">
                <Label>Evaluation Dimensions</Label>
                <Button
                  onClick={() => handleGenerateDimensions(false)}
                  disabled={generatingDimensions || !systemPrompt}
                  variant="ghost"
                  size="sm"
                  className="h-auto py-1"
                  data-testid="regenerate-dimensions-button"
                >
                  {generatingDimensions ? (
                    <><Loader2 className="w-3 h-3 mr-1 animate-spin" />Regenerating...</>
                  ) : (
                    <><RefreshCw className="w-3 h-3 mr-1" />Regenerate</>
                  )}
                </Button>
              </div>
              
              {generatingDimensions ? (
                <div className="p-8 border-2 border-dashed border-primary/30 rounded-lg text-center bg-primary/5">
                  <Loader2 className="w-8 h-8 mx-auto mb-2 text-primary animate-spin" />
                  <p className="text-sm font-medium text-primary mb-1">
                    AI is analyzing your prompt...
                  </p>
                  <p className="text-xs text-muted-foreground">
                    Generating comprehensive evaluation dimensions
                  </p>
                </div>
              ) : dimensionsText ? (
                <>
                  {dimensionAnalysis && (
                    <div className="p-3 bg-primary/5 border border-primary/20 rounded-lg space-y-1 mb-2">
                      <div className="flex items-start space-x-2">
                        <Sparkles className="w-4 h-4 text-primary mt-0.5 flex-shrink-0" />
                        <div className="text-xs space-y-1">
                          <p className="font-medium text-primary">
                            {dimensionAnalysis.prompt_summary}
                          </p>
                          <p className="text-muted-foreground">
                            Complexity: <span className="font-medium capitalize">{dimensionAnalysis.complexity_level}</span> â€¢ 
                            Dimensions: <span className="font-medium">{dimensionsText.split('\n').filter(l => l.trim()).length}</span>
                          </p>
                        </div>
                      </div>
                    </div>
                  )}
                  <Textarea
                    value={dimensionsText}
                    onChange={(e) => setDimensionsText(e.target.value)}
                    rows={12}
                    className="font-mono text-sm"
                    placeholder="Enter evaluation dimensions (format: name: description)&#10;&#10;evidence_grounding: Claims without signal support; overgeneralization&#10;&#10;contextual_relevance: Wrong standards applied; generic findings"
                    data-testid="dimensions-textarea"
                  />
                  <div className="flex items-start space-x-2 text-xs text-muted-foreground">
                    <Info className="w-3 h-3 mt-0.5 flex-shrink-0" />
                    <p>
                      <span className="font-medium">Editable:</span> You can add, edit, or remove dimensions. 
                      Each line will generate a separate evaluation prompt.
                    </p>
                  </div>
                  {dimensions.length > 0 && (
                    <Button
                      variant="ghost"
                      size="sm"
                      onClick={() => setShowDimensionsModal(true)}
                      className="w-full"
                    >
                      <Info className="w-3 h-3 mr-1" />
                      View Details & Descriptions
                    </Button>
                  )}
                </>
              ) : (
                <div className="p-8 border-2 border-dashed border-border rounded-lg text-center">
                  <Sparkles className="w-8 h-8 mx-auto mb-2 text-muted-foreground animate-pulse" />
                  <p className="text-sm text-muted-foreground mb-1">
                    Auto-generating dimensions...
                  </p>
                  <p className="text-xs text-muted-foreground">
                    This happens automatically when you arrive at this step
                  </p>
                </div>
              )}
            </div>

            <div className="grid grid-cols-2 gap-2">
              <Button
                onClick={() => handleGenerateAllEvals(true)}
                disabled={loading || !dimensionsText}
                variant="default"
                className="w-full button-hover"
                data-testid="clear-regenerate-button"
              >
                {loading ? (
                  <><Loader2 className="w-4 h-4 mr-2 animate-spin" />Regenerating...</>
                ) : (
                  <><RefreshCw className="w-4 h-4 mr-2" />Clear & Regenerate All</>
                )}
              </Button>
              
              <Button
                onClick={handleAddMoreDimensions}
                disabled={generatingDimensions}
                variant="outline"
                className="w-full button-hover"
                data-testid="add-more-dimensions-button"
              >
                {generatingDimensions ? (
                  <><Loader2 className="w-4 h-4 mr-2 animate-spin" />Adding...</>
                ) : (
                  <><Sparkles className="w-4 h-4 mr-2" />Add More Dimensions</>
                )}
              </Button>
            </div>
            
            {evalPrompts.length > 0 && (
              <div className="flex items-center justify-between p-2 bg-muted/50 rounded text-xs text-muted-foreground">
                <span>{evalPrompts.length} existing eval prompt{evalPrompts.length !== 1 ? 's' : ''}</span>
                <div className="flex items-center space-x-1">
                  <Button
                    variant="ghost"
                    size="sm"
                    onClick={handleDownloadCSV}
                    disabled={loading}
                    className="h-6 text-xs hover:text-primary"
                    title="Download all prompts as CSV"
                  >
                    <Download className="w-3 h-3 mr-1" />
                    CSV
                  </Button>
                  <Button
                    variant="ghost"
                    size="sm"
                    onClick={handleClearAllEvals}
                    disabled={loading}
                    className="h-6 text-xs hover:text-destructive"
                  >
                    Clear All
                  </Button>
                </div>
              </div>
            )}

            {loading && (
              <div className="p-4 bg-gradient-to-r from-primary/10 to-primary/5 border border-primary/30 rounded-md space-y-2 text-sm animate-pulse">
                <div className="flex items-center space-x-2">
                  <Loader2 className="w-4 h-4 text-primary animate-spin" />
                  <p className="font-medium text-primary">âš¡ Parallel Generating Evaluation Prompts...</p>
                </div>
                <ul className="space-y-1 text-muted-foreground text-xs">
                  <li>â€¢ Processing all {dimensionsText.split('\n').filter(l => l.trim()).length} dimensions simultaneously</li>
                  <li>â€¢ Generating comprehensive eval prompts (600-1500 words each)</li>
                  <li>â€¢ Meta-evaluating quality (1-10 scoring)</li>
                  <li>â€¢ Auto-refining if score {'<'} 8.5 (max 2 attempts per prompt)</li>
                </ul>
                <p className="text-xs text-muted-foreground font-medium">
                  âš¡ Parallel execution is much faster - expect results soon!
                </p>
              </div>
            )}
          </CardContent>
        </Card>

        {/* Generation Status & Results */}
        <Card>
          <CardHeader>
            <CardTitle>Evaluation Prompts</CardTitle>
            {evalPrompts.length > 0 && (
              <div className="flex items-center space-x-2 mt-2">
                <CheckCircle2 className="w-5 h-5 text-green-500" />
                <span className="text-sm font-medium">
                  {evalPrompts.length} evaluation prompt{evalPrompts.length !== 1 ? 's' : ''} generated
                </span>
              </div>
            )}
          </CardHeader>
          <CardContent>
            {loading ? (
              <div className="text-center py-12">
                <Loader2 className="w-12 h-12 mx-auto mb-4 text-primary animate-spin" />
                <p className="font-medium text-primary mb-2">Generating evaluation prompts...</p>
                <p className="text-xs text-muted-foreground">
                  Processing {dimensionsText.split('\n').filter(l => l.trim()).length} dimensions
                </p>
              </div>
            ) : evalPrompts.length === 0 ? (
              <div className="text-center py-12 text-muted-foreground">
                <Sparkles className="w-12 h-12 mx-auto mb-4 opacity-50" />
                <p className="mb-2">No evaluation prompts yet</p>
                <p className="text-xs">
                  Edit dimensions on the left and click "Generate"
                </p>
              </div>
            ) : (
              <div className="space-y-1.5">
                {metaEvaluating && (
                  <div className="p-3 border border-primary/50 bg-primary/5 rounded-md mb-2">
                    <div className="flex items-center space-x-2">
                      <Loader2 className="w-4 h-4 animate-spin text-primary" />
                      <span className="text-xs text-primary font-medium">
                        Running quality checks on generated prompts...
                      </span>
                    </div>
                  </div>
                )}
                {evalPrompts.map((ep) => (
                  <div
                    key={ep.id}
                    onClick={() => {
                      setSelectedEvalPrompt(ep);
                      setShowEvalPromptModal(true);
                    }}
                    className="p-3 border border-border rounded-md hover:border-primary hover:bg-primary/5 transition-colors cursor-pointer group"
                  >
                    <div className="flex items-center justify-between">
                      <div className="flex items-center space-x-2 flex-1 flex-wrap gap-y-1">
                        <Badge variant="outline" className="text-xs">
                          {ep.dimension}
                        </Badge>
                        {metaEvaluations[ep.id] && (
                          <Badge variant="secondary" className="text-xs bg-green-600 text-white">
                            âœ“ Audited
                          </Badge>
                        )}
                        {validationResults[ep.id] && (
                          <Badge
                            variant="secondary"
                            className={`text-xs ${
                              validationResults[ep.id].overall_quality === 'strong' ? 'bg-green-600 text-white' :
                              validationResults[ep.id].overall_quality === 'weak' ? 'bg-red-600 text-white' :
                              'bg-yellow-500 text-white'
                            }`}
                          >
                            {validationResults[ep.id].overall_quality === 'strong' ? 'Validated' :
                             validationResults[ep.id].overall_quality === 'weak' ? 'Needs Refine' :
                             'Moderate'}
                          </Badge>
                        )}
                      </div>
                      <div className="flex items-center space-x-2">
                        <Button
                          variant="ghost"
                          size="sm"
                          className="h-7 px-2 opacity-0 group-hover:opacity-100 transition-opacity"
                          disabled={validating[ep.id]}
                          onClick={(e) => handleValidateEval(ep, e)}
                          title="Validate discrimination & consistency"
                        >
                          {validating[ep.id] ? (
                            <Loader2 className="w-3 h-3 animate-spin" />
                          ) : (
                            <ShieldCheck className="w-3 h-3" />
                          )}
                        </Button>
                        <div className="flex items-center space-x-1">
                          <Star className="w-4 h-4 text-yellow-500 fill-yellow-500" />
                          <span className="font-bold text-sm">{ep.quality_score?.toFixed(1)}</span>
                        </div>
                      </div>
                    </div>
                  </div>
                ))}
              </div>
            )}
          </CardContent>
        </Card>
      </div>

      {/* Suite Analysis Section - Temporarily disabled due to Babel plugin issue */}
      {/* Backend API endpoints are fully functional - test via /api/analyze-overlaps, /api/analyze-coverage, /api/meta-evaluate-suite */}
      {/* {evalPrompts.length >= 2 && (
        <SuiteAnalysis project={project} evalPrompts={evalPrompts} />
      )} */}

      {/* Dimensions Details Modal */}
      <Dialog open={showDimensionsModal} onOpenChange={setShowDimensionsModal}>
        <DialogContent className="max-w-4xl max-h-[80vh] overflow-y-auto">
          <DialogHeader>
            <DialogTitle className="flex items-center space-x-2">
              <Sparkles className="w-5 h-5 text-primary" />
              <span>AI-Generated Evaluation Dimensions</span>
            </DialogTitle>
            <DialogDescription>
              {dimensionAnalysis?.prompt_summary}
            </DialogDescription>
          </DialogHeader>

          {dimensionAnalysis && (
            <div className="space-y-4 mt-4">
              <div className="flex items-center space-x-4 p-3 bg-muted rounded-lg">
                <div>
                  <p className="text-sm font-medium">Complexity Level</p>
                  <p className="text-2xl font-bold text-primary capitalize">
                    {dimensionAnalysis.complexity_level}
                  </p>
                </div>
                <div className="border-l pl-4">
                  <p className="text-sm font-medium">Recommended Dimensions</p>
                  <p className="text-2xl font-bold text-primary">
                    {dimensionAnalysis.recommended_dimension_count}
                  </p>
                </div>
                <div className="border-l pl-4">
                  <p className="text-sm font-medium">Generated</p>
                  <p className="text-2xl font-bold text-primary">
                    {dimensions.length}
                  </p>
                </div>
              </div>

              <div className="space-y-3">
                <h3 className="font-semibold text-sm">Evaluation Dimensions</h3>
                {dimensions.map((dim, idx) => (
                  <div
                    key={idx}
                    className="border border-border rounded-lg p-4 space-y-2 hover:border-primary/50 transition-colors"
                  >
                    <div className="flex items-start justify-between">
                      <div className="flex-1">
                        <div className="flex items-center space-x-2 mb-1">
                          <h4 className="font-medium">{dim.name}</h4>
                          <Badge variant={dim.priority === 'High' ? 'default' : 'secondary'}>
                            {dim.priority}
                          </Badge>
                          <Badge variant="outline">{dim.category}</Badge>
                        </div>
                        <p className="text-sm text-muted-foreground mb-2">
                          {dim.description}
                        </p>
                        <div className="mt-2 p-2 bg-muted rounded text-xs">
                          <p className="font-medium mb-1">Example Test:</p>
                          <p className="text-muted-foreground">{dim.example_test}</p>
                        </div>
                      </div>
                      <Button
                        size="sm"
                        variant={dimension === dim.name ? 'default' : 'outline'}
                        onClick={() => {
                          setDimension(dim.name);
                          toast.success(`Selected: ${dim.name}`);
                        }}
                        className="ml-4"
                      >
                        {dimension === dim.name ? (
                          <><CheckCircle2 className="w-4 h-4 mr-1" />Selected</>
                        ) : (
                          'Select'
                        )}
                      </Button>
                    </div>
                  </div>
                ))}
              </div>

              <div className="flex justify-end space-x-2 pt-4 border-t">
                <Button
                  variant="outline"
                  onClick={() => setShowDimensionsModal(false)}
                >
                  Close
                </Button>
                <Button
                  onClick={() => {
                    setShowDimensionsModal(false);
                    if (dimension) {
                      handleGenerate();
                    } else {
                      toast.error('Please select a dimension first');
                    }
                  }}
                  disabled={!dimension}
                >
                  Generate Eval Prompt for Selected
                </Button>
              </div>
            </div>
          )}
        </DialogContent>
      </Dialog>

      {/* View Eval Prompt Modal */}
      <Dialog open={showEvalPromptModal} onOpenChange={setShowEvalPromptModal}>
        <DialogContent className="max-w-4xl max-h-[85vh]">
          <DialogHeader>
            <DialogTitle className="flex items-center justify-between">
              <div className="flex items-center space-x-2">
                <Star className="w-5 h-5 text-yellow-500 fill-yellow-500" />
                <span>{selectedEvalPrompt?.dimension}</span>
              </div>
              {selectedEvalPrompt && (
                <div className="flex items-center space-x-2">
                  <Badge variant="secondary">
                    Quality: {selectedEvalPrompt.quality_score?.toFixed(1)}/10
                  </Badge>
                  {selectedEvalPrompt.quality_score >= 8.5 && (
                    <Badge variant="default" className="bg-green-600">
                      âœ“ Passes Quality Gate
                    </Badge>
                  )}
                  <Badge variant="outline">
                    {selectedEvalPrompt.refinement_count || 0} refinements
                  </Badge>
                </div>
              )}
            </DialogTitle>
            <DialogDescription>
              Evaluation prompt for assessing {selectedEvalPrompt?.dimension?.toLowerCase()}
            </DialogDescription>
          </DialogHeader>

          {selectedEvalPrompt && (
            <div className="space-y-4 mt-4 overflow-y-auto max-h-[calc(85vh-120px)]">
              {/* Meta-Evaluation Feedback */}
              {metaEvaluations[selectedEvalPrompt.id] && (
                <div className="p-4 bg-gradient-to-br from-green-50 to-emerald-50 dark:from-green-950/20 dark:to-emerald-950/20 rounded-lg border-2 border-green-500/30">
                  <div className="flex items-center space-x-2 mb-3">
                    <CheckCircle2 className="w-5 h-5 text-green-600" />
                    <h3 className="font-semibold text-green-900 dark:text-green-100">
                      Meta-Evaluation Report
                    </h3>
                    <Badge variant="default" className="bg-green-600 ml-auto">
                      Quality Audited
                    </Badge>
                  </div>
                  <div className="prose prose-sm max-w-none dark:prose-invert bg-white/50 dark:bg-black/20 p-4 rounded border border-green-200 dark:border-green-800">
                    <div className="text-sm whitespace-pre-wrap leading-relaxed text-gray-800 dark:text-gray-200">
                      {metaEvaluations[selectedEvalPrompt.id].analysis}
                    </div>
                  </div>
                  <div className="mt-3 text-xs text-green-700 dark:text-green-300">
                    ðŸ’¡ This feedback was generated by analyzing the relationship between your system prompt and evaluation prompt using Meta-Eval Expert framework
                  </div>
                </div>
              )}

              {/* Validation Results Section */}
              {validationResults[selectedEvalPrompt.id] && (
                <div className="p-4 rounded-lg border-2 space-y-3" style={{
                  borderColor: validationResults[selectedEvalPrompt.id].overall_quality === 'strong' ? 'rgb(34, 197, 94)' :
                               validationResults[selectedEvalPrompt.id].overall_quality === 'weak' ? 'rgb(239, 68, 68)' : 'rgb(234, 179, 8)',
                  backgroundColor: validationResults[selectedEvalPrompt.id].overall_quality === 'strong' ? 'rgba(34, 197, 94, 0.05)' :
                                   validationResults[selectedEvalPrompt.id].overall_quality === 'weak' ? 'rgba(239, 68, 68, 0.05)' : 'rgba(234, 179, 8, 0.05)'
                }}>
                  <div className="flex items-center justify-between">
                    <div className="flex items-center space-x-2">
                      <ShieldCheck className="w-5 h-5" />
                      <h3 className="font-semibold text-sm">Eval Validation Results</h3>
                    </div>
                    <Badge className={
                      validationResults[selectedEvalPrompt.id].overall_quality === 'strong' ? 'bg-green-600 text-white' :
                      validationResults[selectedEvalPrompt.id].overall_quality === 'weak' ? 'bg-red-600 text-white' : 'bg-yellow-500 text-white'
                    }>
                      {validationResults[selectedEvalPrompt.id].overall_quality}
                    </Badge>
                  </div>

                  <div className="grid grid-cols-2 gap-4">
                    {/* Discrimination */}
                    <div className="p-3 bg-background rounded border">
                      <div className="flex items-center justify-between mb-2">
                        <span className="text-xs font-medium">Discrimination Power</span>
                        <Badge className={getValidationBadgeColor(validationResults[selectedEvalPrompt.id].discrimination?.rating)}>
                          {validationResults[selectedEvalPrompt.id].discrimination?.rating}
                        </Badge>
                      </div>
                      <div className="text-lg font-bold">
                        {validationResults[selectedEvalPrompt.id].discrimination?.discrimination_score}
                      </div>
                      <div className="text-xs text-muted-foreground mt-1">
                        Easy avg: {validationResults[selectedEvalPrompt.id].discrimination?.avg_easy_score} |
                        Hard avg: {validationResults[selectedEvalPrompt.id].discrimination?.avg_hard_score}
                      </div>
                    </div>

                    {/* Consistency */}
                    <div className="p-3 bg-background rounded border">
                      <div className="flex items-center justify-between mb-2">
                        <span className="text-xs font-medium">Score Consistency</span>
                        <Badge className={getValidationBadgeColor(validationResults[selectedEvalPrompt.id].consistency?.rating)}>
                          {validationResults[selectedEvalPrompt.id].consistency?.rating}
                        </Badge>
                      </div>
                      <div className="text-lg font-bold">
                        {validationResults[selectedEvalPrompt.id].consistency?.consistency_score}
                      </div>
                      <div className="text-xs text-muted-foreground mt-1">
                        Avg std dev: {validationResults[selectedEvalPrompt.id].consistency?.avg_std_dev} |
                        Trials: {validationResults[selectedEvalPrompt.id].consistency?.num_trials}
                      </div>
                    </div>
                  </div>

                  {/* Refine button - only if not strong */}
                  {validationResults[selectedEvalPrompt.id].overall_quality !== 'strong' && (
                    <Button
                      onClick={() => handleRefineWithEvidence(selectedEvalPrompt)}
                      disabled={refining}
                      className="w-full bg-orange-600 hover:bg-orange-700 text-white"
                    >
                      {refining ? (
                        <><Loader2 className="w-4 h-4 mr-2 animate-spin" />Refining with evidence...</>
                      ) : (
                        <><Wand2 className="w-4 h-4 mr-2" />Refine Based on Validation Results</>
                      )}
                    </Button>
                  )}
                </div>
              )}

              {/* Validate button if no results yet */}
              {!validationResults[selectedEvalPrompt.id] && (
                <Button
                  variant="outline"
                  onClick={(e) => handleValidateEval(selectedEvalPrompt, e)}
                  disabled={validating[selectedEvalPrompt.id]}
                  className="w-full"
                >
                  {validating[selectedEvalPrompt.id] ? (
                    <><Loader2 className="w-4 h-4 mr-2 animate-spin" />Validating discrimination & consistency...</>
                  ) : (
                    <><ShieldCheck className="w-4 h-4 mr-2" />Validate Eval Quality</>
                  )}
                </Button>
              )}

              {/* Eval Prompt Content */}
              <div>
                <div className="flex items-center justify-between mb-2">
                  <h3 className="font-semibold text-sm">Evaluation Prompt</h3>
                  {!metaEvaluations[selectedEvalPrompt.id] && (
                    <Badge variant="outline" className="text-xs">
                      Not yet audited
                    </Badge>
                  )}
                </div>
                <div className="max-h-[40vh] overflow-y-auto p-4 bg-muted rounded-lg border">
                  <pre className="text-xs whitespace-pre-wrap font-mono leading-relaxed">
                    {selectedEvalPrompt.content}
                  </pre>
                </div>
              </div>

              {/* Human Feedback Section */}
              <div className="border-t pt-4">
                <h3 className="font-semibold text-sm mb-3">Help Us Improve</h3>
                <EvalFeedback
                  evalPrompt={selectedEvalPrompt}
                  userId={settings?.session_id}
                  onFeedbackSubmitted={(rating, comment) => {
                    // Optionally reload eval prompts to show updated rating
                    loadEvalPrompts();
                  }}
                />
              </div>

              <div className="flex items-center justify-between pt-4 border-t">
                <div className="text-xs text-muted-foreground">
                  <p>Template variables will be auto-detected when generating test data</p>
                </div>
                <div className="flex space-x-2">
                  <Button
                    variant="outline"
                    onClick={() => {
                      navigator.clipboard.writeText(selectedEvalPrompt.content);
                      toast.success('Copied to clipboard!');
                    }}
                  >
                    Copy
                  </Button>
                  <Button
                    variant="outline"
                    onClick={() => handleAddToMaxim(selectedEvalPrompt.id)}
                    disabled={addingToMaxim || !settings?.maxim_api_key}
                  >
                    {addingToMaxim ? (
                      <>
                        <Loader2 className="w-4 h-4 mr-2 animate-spin" />
                        Adding...
                      </>
                    ) : (
                      'Add to Maxim'
                    )}
                  </Button>
                  <Button onClick={() => setShowEvalPromptModal(false)}>
                    Close
                  </Button>
                </div>
              </div>
            </div>
          )}
        </DialogContent>
      </Dialog>
    </div>
  );
}