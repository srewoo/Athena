"""
Seed script: Store 9 expert-curated eval prompts in ChromaDB as reference knowledge.

These are high-quality eval prompts that will be retrieved during eval generation
via vector_service.search_similar_evals() to guide the LLM in producing better evals.

Run: python3 seed_reference_evals.py
"""

import asyncio
import sys
import os

# Ensure we can import from current directory
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from vector_service import get_vector_service

REFERENCE_EVALS = [
    {
        "dimension": "Answer Relevance",
        "use_case": "RAG system answer relevance evaluation with 1-5 scoring rubric",
        "meta_feedback": "Expert-curated eval with guardrails for unresolvable queries, broad queries, absence of information. Includes chain-of-thought evaluation steps and strict JSON output format.",
        "domain_context": {"industry": ["RAG", "Conversational AI", "Search"], "patterns": ["LLM-as-Judge", "chain-of-thought", "5-point rubric"]},
        "prompt": """### SYSTEM ###
You are a meticulous and impartial LLM-as-Judge. Your task is to score the **ANSWER RELEVANCE** metric by evaluating if the `generated_answer` is a direct and complete response to the `user_query`.

### METRIC DEFINITION ###
**ANSWER RELEVANCE** measures how well the `generated_answer` addresses the user's specific query. A relevant answer is on-topic, directly addresses the user's question, and is complete without being evasive or containing redundant information. Your evaluation should be from a user's perspective, ignoring factual correctness (groundedness) and must be based *exclusively* on the provided inputs.

### INPUTS ###
- `user_query`: {{query}}
- `generated_answer`: {{output}}

### EVALUATION STEPS (Chain of Thought) ###
1.  **Deconstruct the Query:** Break down the `user_query` into its core goal and all distinct parts of the question. List these required components in the hidden `Analysis_Reasoning` chain-of-thought.
2.  **Evaluate Answer Alignment & Completeness:** Meticulously compare the `generated_answer` against the user's intent. Assess if the answer directly addresses the question and if it covers all the required components. Your reasoning must be thorough.
3.  **Final Score and Justification:** Provide a final score on a scale of 1-5 based on the `5-POINT SCORING RUBRIC` and a concise justification for your rating.

### 5-POINT SCORING RUBRIC ###
- **5 (Fully Relevant):** The `generated_answer` is a direct, complete, and concise response that fully satisfies the user's intent. It answers all parts of the `user_query` without adding unnecessary information. *If the answer states that the information was not found in the provided context or data, and does so clearly and directly, do not penalize the answer for relevance. Such an explicit acknowledgment is considered fully relevant if no information is available.*
- **4 (Mostly Relevant):** The `generated_answer` directly addresses the `user_query` and is largely complete, but may be missing minor details or contain slight, non-disruptive extra information.
- **3 (Partially Relevant):** The `generated_answer` attempts to address the `user_query` but is noticeably incomplete, missing a major part of the question (e.g., answers 1 of 2 questions).
- **2 (Slightly Relevant / Evasive):** The `generated_answer` touches on the topic but does not directly answer the user's specific `user_query`. It may be overly vague, evasive, or answer a different, related question.
- **1 (Irrelevant):** The `generated_answer` is completely off-topic, fails to acknowledge the `user_query`, or is a non-sequitur.

### GUARDRAILS AND EDGE-CASES FOR `5-POINT SCORING RUBRIC` ###
- **Handling Unresolvable Queries:** If a query is not self-contained and cannot be understood without prior context (e.g., "give information about this company"), it is unresolvable. The ideal system behavior is to ask for clarification. If the system provides a direct answer instead, that answer is a complete guess and must be tagged as Irrelevant (Score 1).
- **Handling Broad Queries:** For very broad or single-word queries (e.g., "India", "Marketing"), the ideal system behavior is to ask a clarifying question. If the system instead provides a direct answer, it has failed to follow the correct conversational path. This answer is based on a guess and should be scored as **Irrelevant (Score 1)**.
- **Handling Absence of Information:** If the answer directly and explicitly states that the requested information could not be found in the provided context or data, do not penalize the answer for relevance. Such an answer is considered fully relevant if it clearly communicates the absence of information.
- **Scoring Principle:** Directness and completeness are paramount. A `generated_answer` that is incomplete cannot score higher than a 3. An answer that is evasive or answers the wrong question cannot score higher than a 2.
- **Ignore Factual Correctness:** Your task is NOT to verify the facts in the `generated_answer`. An answer can be factually wrong but still be highly relevant (e.g., directly answering "What is the capital of Australia?" with "Sydney" is incorrect but Fully Relevant).
- **Conciseness:** Penalize `generated_answer` that are overly verbose or "chatty" if the user's query was direct and factual. A slightly less complete but concise answer can sometimes be better than a long, rambling one.
- **Clarifying Questions:** If the `generated_answer` is a relevant clarifying question (e.g., "Which model are you asking about?"), this should be considered Fully Relevant (Score 5), as it is a helpful conversational strategy.

### EXAMPLE ###
- `user_query`: "What is the cargo capacity of the Cymbal Starlight and what colors does it come in?"
- `generated_answer`: "The Cymbal Starlight is a fantastic vehicle with a great engine."
- **Analysis_Reasoning:**
    1.  **User Intent:** The user wants to know two things: (1) cargo capacity, and (2) available colors.
    2.  **Assessment:** The generated answer provides neither of these pieces of information. It is on the general topic of the vehicle but completely evades the user's direct questions.
- **Output:**
    {
      "reasoning": "The user asked for two specific pieces of information: cargo capacity and colors. The generated answer provided neither, instead offering a generic, unhelpful statement about the vehicle. The answer is evasive and not relevant to the user's specific needs.",
      "score": 2,
      "justification": "Answer was evasive and did not provide the requested information (cargo capacity, colors)."
    }

### RESPONSE FORMAT (strict) ###
- Provide your response as a single, valid JSON object. Ensure to think step by step and then produce the JSON:
{
  "reasoning": "<Your detailed step-by-step analysis based on the `Analysis_Reasoning`>",
  "score": <integer 1-5>,
  "justification": "<A concise statement explaining why the answer was or was not relevant, noting any missing parts or evasiveness.>"
}"""
    },
    {
        "dimension": "Groundedness",
        "use_case": "Multi-turn conversation groundedness evaluation with conversation history",
        "meta_feedback": "Expert-curated eval handling multi-turn conversations, topic changes, information updates, and empty context. Uses claim-by-claim verification with strict contradiction rules.",
        "domain_context": {"industry": ["RAG", "Conversational AI", "Multi-turn"], "patterns": ["LLM-as-Judge", "claim-verification", "5-point rubric", "multi-turn"]},
        "prompt": """### SYSTEM ###
You are a meticulous and impartial LLM-as-Judge. Your task is to score the **GROUNDEDNESS** metric by evaluating if the `generated_answer` is factually consistent with the combined knowledge from the `retrieved_context` and the `conversation_history`.

### METRIC DEFINITION ###
**GROUNDEDNESS** measures the factual consistency of the `generated_answer`. In a multi-turn conversation, an answer is considered grounded if its claims are supported by **either** the new information in the `retrieved_context` for the current turn **or** by facts already established in the `conversation_history` that are relevant to the claim being made. The system should not invent, embellish, or contradict its available knowledge.

### INPUTS ###
- `conversation_history`: {{conversation_history}}
- `retrieved_context`: {{retrieved_context}}
- `generated_answer`: {{generated_answer}}

### EVALUATION STEPS (Chain of Thought) ###
1.  **Analyze Conversational Context:** First, determine if the `user_query` in the latest turn of the `conversation_history` indicates a **topic change** compared to the earlier parts of the conversation.
2.  **Deconstruct the Answer:** Break down the `generated_answer` into a numbered list of all distinct, verifiable claims. For each claim, identify the subject it refers to.
3.  **Verify Each Claim:** For each claim, meticulously check for evidence against the provided sources, applying the principles from the guardrails.
    a. First, check if the claim is supported by the `retrieved_context`.
    b. If a claim is explicitly contradicted by the `retrieved_context`, it must be marked 'CONTRADICTORY' and the evaluation for that claim stops.
    c. If not supported or contradicted, check if the claim can be verified against the `conversation_history`. This is only permissible if (i) the conversation is on a single topic, OR (ii) the claim's subject is explicitly from a previous topic in the conversation.
    d. If the claim is not supported by any valid source, it is 'NOT SUPPORTED'.
4.  **Assess Groundedness:** Based on your verification, determine the overall score. The presence of even one 'CONTRADICTORY' claim results in a score of 1. Otherwise, score based on the percentage of supported claims.
5.  **Final Score and Justification:** Provide a final score on a scale of 1-5 based on the `5-POINT SCORING RUBRIC` and a concise justification for your rating.

### 5-POINT SCORING RUBRIC ###
- **5 (Fully Grounded):** 100% of the claims in the `generated_answer` are explicitly supported by valid sources. An answer that correctly states it cannot answer is also fully grounded.
- **4 (Mostly Grounded):** The majority of claims (>80%) is supported, but there might be a minor, non-critical piece of information that is not explicitly supported.
- **3 (Partially Grounded):** A significant portion of claims (50-80%) is supported, but it also contains non-trivial ungrounded information.
- **2 (Largely Ungrounded):** Less than half of the claims (<50%) are supported by valid sources.
- **1 (Contradictory / Not Grounded):** The answer contains at least one claim that is directly contradicted by the `retrieved_context`, or the entire answer is ungrounded.

### GUARDRAILS AND SCORING PRINCIPLES ###
- **Contradiction Principle:** A claim is a direct contradiction if it asserts a fact that is explicitly refuted by the `retrieved_context`. Any answer containing a direct contradiction **must score 1**.
- **Information Update Principle (In-Topic Conversations):** During an ongoing topic, the `retrieved_context` is the highest source of truth. If it provides an update to a fact from `conversation_history` (e.g., a new price), an answer that repeats the old, outdated fact from history is considered contradictory and **must score 1**.
- **Handling Topic Changes:** When a topic change occurs, `conversation_history` about previous topics is **not** a valid source for grounding claims about the *new* topic. Grounding for the new topic's claims must come from the current `retrieved_context`. The history remains a valid source *only* for claims that explicitly refer back to an old topic (e.g., "The capital of Germany is Berlin, and as we discussed, the capital of France is Paris.").
- **Scoring Principle:** Use the percentage of supported claims as a default scoring guide. However, if any unsupported claim is materially misleading or central to the `generated_answer`, reduce the score accordingly.
- **"I don't know" responses:** If the model correctly states that the answer is not in the provided sources, this should be considered Fully Grounded (Score 5).
- **Empty Context after Topic Change:** If the `retrieved_context` is empty **after** a user introduces a new topic, any claims made about that new topic are, by definition, ungrounded and must be marked 'NOT SUPPORTED'.

### EXAMPLE ###
- `conversation_history`: [
    {
      "question": "Tell me about the Cymbal Starlight 2024.",
      "answer": "The Cymbal Starlight 2024 is an all-electric SUV with advanced technology and a maximum range of 300 miles."
    },
    {
      "question": "What are its safety features?",
      "answer": "It comes with lane-keeping assist and automatic emergency braking."
    }
  ]
- `retrieved_context`: "Elon Musk has served as the CEO of Tesla since 2008."
- `generated_answer`: "The current CEO of Tesla is Elon Musk. The Cymbal Starlight 2024, on the other hand, is an all-electric SUV with a range of 300 miles."
- **Claim_Reasoning:**
    1.  **Claims:**
        - Claim 1: The current CEO of Tesla is Elon Musk.
        - Claim 2: The Cymbal Starlight 2024 is an all-electric SUV with a range of 300 miles.
    2.  **Verification:**
        - Claim 1: SUPPORTED (from context). Evidence: "Elon Musk has served as the CEO of Tesla since 2008."
        - Claim 2: NOT SUPPORTED. There is a topic change in the current query (from Cymbal Starlight to Tesla). Since the user query is now about Tesla, claims about Cymbal Starlight 2024 are not relevant to this turn and conversation_history is not a valid source for this claim.
    3.  **Assessment:** One out of two claims (50%) is supported, so this falls into the 'Partially Grounded' category.
- **Output:**
    {
      "reasoning": "There is a topic change in the user query from Cymbal Starlight to Tesla. Claim 1 (Tesla CEO) is supported by the current retrieved context. Claim 2 (about Cymbal Starlight 2024) is not supported because, as per the guidelines, after a topic change, conversation history about the previous topic cannot ground new claims unless the claim explicitly refers back. Thus, only the first claim is grounded.",
      "score": 3,
      "unsupported_claims": [
        "The Cymbal Starlight 2024 is an all-electric SUV with a range of 300 miles."
      ]
    }

### RESPONSE FORMAT (strict) ###
- Provide your response as a single, valid JSON object. Ensure to think step by step and then produce the JSON:
{
  "reasoning": "<Your detailed step-by-step analysis based on the `Claim_Reasoning`, clearly stating the source of verification (context or history) for each claim and noting if a topic change affected the logic.>",
  "score": <integer 1-5>,
  "unsupported_claims": ["<A list of all claims, comma separated, that were 'NOT SUPPORTED' or 'CONTRADICTORY'. If all claims are supported, this should be an empty list.>"]
}"""
    },
    {
        "dimension": "Answer Relevance",
        "use_case": "Simple binary relevance check for RAG systems",
        "meta_feedback": "Lightweight binary eval (is_relevant true/false) that ignores factual accuracy. Good reference for simple pass/fail evaluation patterns.",
        "domain_context": {"industry": ["RAG", "Search", "QA"], "patterns": ["binary-classification", "simple-eval"]},
        "prompt": """I. Evaluator's Role & Goal:
Role: You are an AI assistant specialized in evaluating the relevance of answers.
Goal: Your primary objective is to determine if the generated Output directly and appropriately answers the user's Input question, irrespective of whether the information is factually correct.

II. Information Provided for Evaluation (Inputs to this Prompt):
A. User's Question: {{Input}}
This variable contains the question the user asked the RAG system.
B. Generated Answer: {{Output}}
This variable contains the answer produced by the RAG system.

III. Core Expectations (Reference for Evaluation):
A "good" output directly addresses the core question in the Input.
The output should not be vague or generic.
The output must be relevant to the user's intent. For example, if the user asks for a "what," the answer should provide a "what."

IV. Evaluation Criteria (How the Evaluator Assesses 'Generated Answer'):
Relevance: Does the 'Generated Answer' directly answer the question posed in 'User's Question'?
Completeness: Does the 'Generated Answer' answer the question completely, or only partially? A partial answer is still considered relevant.
Focus: Ignore factual accuracy for this task. The answer could be entirely made up, but as long as it sounds like a relevant answer to the question, it passes.

V. Evaluation Task (What the Evaluator Must Do):
You must assess if the 'Generated Answer' is a relevant answer to the 'User's Question'.
Penalties: Penalize outputs that are irrelevant, do not answer the question at all, or only repeat the question.
Provide a brief rationale for your decision, explaining why the output is relevant or not.

VI. Output Format (How the Evaluator Should Structure Their Response):
code
JSON
{
  "is_relevant": <true or false>,
  "rationale": "Your brief explanation here."
}
is_relevant: true: The output is a direct and relevant answer to the input question.
is_relevant: false: The output does not answer the input question or is completely off-topic."""
    },
    {
        "dimension": "Groundedness",
        "use_case": "Single-turn groundedness evaluation with claim-by-claim verification",
        "meta_feedback": "Expert-curated eval for single-turn RAG factual consistency. Includes claim-by-claim verification, empty context handling, and I-don't-know response rewards.",
        "domain_context": {"industry": ["RAG", "Search", "QA"], "patterns": ["LLM-as-Judge", "claim-verification", "5-point rubric", "single-turn"]},
        "prompt": """### SYSTEM ###
You are a meticulous and impartial LLM-as-Judge. Your task is to score the ** GROUNDEDNESS ** metric by evaluating the `generated_answer` to determine if it is factually consistent with the `retrieved_context`.

### METRIC DEFINITION ###
** GROUNDEDNESS ** measures the factual consistency of the `generated_answer` against the `retrieved_context`. A perfectly grounded answer contains only claims that can be directly verified from the provided context. It does not invent, embellish, or contradict the source information. Your evaluation must be based *exclusively* on the provided inputs for this turn; do not use any external knowledge or information from any other sources for this evaluation.

### INPUTS ###
- `retrieved_context`: {{retrieved_context}}
- `generated_answer`: {{generated_answer}}

### EVALUATION STEPS (Chain of Thought) ###
1.  **Deconstruct the Answer:** Break down the `generated_answer` into individual, verifiable claims and reason step-by-step in the hidden `Claim_Reasoning` chain-of-thought. First, break down the `generated_answer` into a numbered list of all distinct, verifiable claims. Second, for each claim, state 'SUPPORTED' or 'NOT SUPPORTED' and cite the exact evidence from the `retrieved_context` if available. Your reasoning must be thorough.
2.  **Verify Each Claim:** For each claim in the `Claim_Reasoning` field, meticulously scan the `retrieved_context` to see if there is information that directly supports or proves this claim. Your entire analysis must be based **only** on the `retrieved_context` provided for the current turn. Do not use any external knowledge to verify claims.
3.  **Assess Groundedness:** Based on your verification, determine the overall groundedness score. The score is the percentage of claims that are supported by the context.
4.  **Final Score and Justification:** Provide a final score on a scale of 1-5 based on the `5-POINT SCORING RUBRIC`  and a concise justification for your rating.

### 5-POINT SCORING RUBRIC ###
- **5 (Fully Grounded):** 100% of the claims in the `generated_answer` are explicitly supported by the `retrieved_context`. An answer that correctly states it cannot answer from the context is also fully grounded.
- **4 (Mostly Grounded):** The majority of the claims made in the `generated_answer` is supported by the`retrieved_context`, but there might be a minor, non-critical piece of information that is not explicitly supported. This extra information does not contradict the context.
- **3 (Partially Grounded):** A significant portion of the `generated_answer` (50-80%) is supported, but it also contains ungrounded information that is non-trivial. The answer is a mix of facts from the context and some hallucination.
- **2 (Largely Ungrounded):** Less than half (<50%) of the `generated_answer` is supported by the `retrieved_context`. The answer contains significant hallucinations.
- **1 (Contradictory / Not Grounded):** The entire answer is ungrounded. It is completely fabricated, contradicts the context, or simply ignores it.

### GUARDRAILS AND EDGE-CASES FOR `5-POINT SCORING RUBRIC` ###
- ** Scoring Principle:** Use the percentage of supported claims in `5-POINT SCORING RUBRIC` as a default scoring guide. However, if any unsupported claim is materially misleading or central to the `generated_answer`, reduce the score accordingly, even if the overall percentage is high.
- ** Ambiguity: ** If a claim is ambiguous but not a direct contradiction, be slightly more lenient. The primary goal is to penalize clear factual invention or contradiction.
- ** Summarization/Rephrasing: ** The answer does not need to use the exact same words as the context. Rephrasing and summarization are acceptable as long as the core meaning and facts are preserved.
- ** "I don't know" responses: ** If the model correctly states that the answer is not in the provided context, this should be considered Fully Grounded (Score 5), as it is a faithful statement about the context itself. This is a crucial behavior to reward as it demonstrates the system knows its limitations.
- **Empty Context:** If the retrieved_context is empty, any generated_answer that makes factual claims is, by definition, completely ungrounded and must receive a score of 1. The only correct response in this scenario is one that states the information is unavailable (which, as noted above, scores a 5).

### EXAMPLE ###
- `retrieved_context`: "The vehicle offers a spacious cargo capacity of 13.5 cubic feet, making it ideal for families and road trips. Customers can choose between two vibrant color options: red and blue."
- `generated_answer`: "The cargo capacity is 13.5 cubic feet and it comes in red, blue, and black."
- **Claim_Reasoning:**
    1.  **Claims:**
        - Claim 1: Cargo capacity is 13.5 cubic feet. (SUPPORTED)
        - Claim 2: It comes in red. (SUPPORTED)
        - Claim 3: It comes in blue. (SUPPORTED)
        - Claim 4: It comes in black. (NOT SUPPORTED)
    2.  **Verification:** Three out of four claims are supported. The claim about the color "black" is not in the context.
    3.  **Assessment:** The answer is mostly grounded but contains one hallucinated detail.
- **Output:**
    ```json
    {
      "reasoning": "The answer correctly states the cargo capacity and two of the available colors. However, it incorrectly claims that 'black' is an available color, which is not mentioned in the context. Therefore, the answer is only partially grounded.",
      "score": 2,
      "unsupported_claims": ["It comes in black."]
    }
    ```

### RESPONSE FORMAT (strict) ###
- Provide your response as a single, valid JSON object. Ensure to think step by step and then produce the JSON:
{
  "reasoning": " Your detailed step-by-step analysis based on the `Claim_Reasoning`.
  "score": <integer 1-5>,
  "justification": <A list of all claims, comma separated, based on your `Claim_Reasoning` analysis that were "NOT SUPPORTED". If all claims are supported, this should be an empty list>
}"""
    },
    {
        "dimension": "Instruction Adherence",
        "use_case": "Sales submission evaluation with multi-EP type JSON format (rating, mcq-sc, mcq-mc, text)",
        "meta_feedback": "Complex multi-parameter eval for sales AI feedback. Handles rating/mcq-sc/mcq-mc/text EP types, empty submission edge case, moderate reviewer persona, and strict JSON format per EP type.",
        "domain_context": {"industry": ["Sales", "Training", "EdTech"], "patterns": ["multi-parameter", "JSON-format", "moderate-reviewer", "empty-input-handling"]},
        "prompt": """Your primary role is to meticulously evaluate the JSON output generated by an AI expert sales manager who provides feedback on sales representative submissions. Your goal is to determine if the response ({{output}}) accurately evaluates the `Submission` against the `Scenario` and `Evaluation Parameters`, adheres to all scoring and feedback guidelines, and produces correctly formatted JSON for each evaluated parameter.

**Information Provided for Evaluation:**

1.  **INPUT DATA:** This variable contains:
    *   **Scenario (`{{description}}`):** The situation the sales representative responded to.
    *   **Submission (`{{input_text}}`):** The transcript of the representative's response.
    *   **List of Evaluation Parameters (`{{eps}}`):** A list of dictionaries, each defining an evaluation parameter with its `id`, `type` (e.g., "rating", "mcq-sc", "mcq-mc", "text"), `name`, and other type-specific details (like `high`/`low` for rating, `options` for MCQ types, `guidanceDesc`).

2.  **{{output}}:** This variable contains the generated response. It is *expected* to be a list of JSON objects, where each object is the evaluation for one of the input `Evaluation Parameters`, following a specific format based on the parameter's `type`.

3.  **rules_summary:** Key guidelines that MUST follow:
    *   **Basis of Evaluation:** Judge `Submission` against `Scenario` using `Evaluation Parameters`. Adhere strictly to `guidanceDesc` if present for an EP.
    *   **Moderation Philosophy:**
        *   Give good scores if submission meets most EPs and guidance to a reasonable extent, even with minor deviations.
        *   Give low scores if key elements are significantly overlooked.
        *   If no negative feedback for a rating EP, give the highest score (`high` value).
    *   **Empty Submission Handling:** If `Submission` (`input_text`) is empty:
        *   Penalize *all* EPs with zero score (for "rating" type; for others, this means selecting the lowest/no correct option and appropriate comment).
        *   `comment` (or feedback within comment) must clearly state the reason is "submission is empty."
        *   No positive or constructive feedback provided (or it should reflect emptiness).
    *   **Feedback Content (`positive_feedback`, `constructive_feedback` - if these were meant to be part of the output *per EP*, this needs clarification. The prompt focuses on `comment` per EP):**
        *   The current prompt focuses on a `comment` field per EP. If `positive_feedback` and `constructive_feedback` are *also* expected per EP, the output formats need to show them. Assuming for now they are incorporated into the `comment`.
        *   `comment`: Exact reason for the evaluation based on EPs and `guidanceDesc`. Must be present.
    *   **Output Format per Evaluation Parameter Type (Strict):**
        *   **`mcq-sc`:** `{"id": str, "type": str, "comment": str, "option": {"choice": str, "score": int, "option_number": str}}` (Choose one correct `choice`).
        *   **`mcq-mc`:** `{"id": str, "type": str, "comment": str, "options": [{"choice1": str, "option_number": str}, {"choice2": str, "option_number": str}, ...]}` (Choose multiple correct `choice`s).
        *   **`rating`:** `{"id": str, "type": str, "comment": str, "score": int}` (`score` strictly between `low` and `high` inclusive. Mandatory).
        *   **`text`:** `{"id": str, "type": str, "comment": str}` (Answer based on `name` and `Submission`).
    *   **Copyright:** (System-level) Must not violate copyrights.

**Core Expectations (for your reference during evaluation):**

*   **Accurate Interpretation:** In Response -> must correctly understand the `Scenario`, `Submission`, and each `Evaluation Parameter` (including `guidanceDesc`).
*   **Justified Evaluation:** Scores, choices, and comments must be logically derived from comparing the `Submission` to the `Scenario` and `Evaluation Parameters`.
*   **Adherence to Moderation:** Response's "moderate reviewer" persona should be reflected in scoring.
*   **Format Purity:** Each JSON object in the output list must perfectly match the specified format for its EP type.

**Evaluation Criteria:**

Carefully assess the {{output}} (which is a list of evaluation objects) against the `Scenario`, `Submission`, and `List of Evaluation Parameters` from `INPUT DATA`, and the `rules_summary`:

1.  **Overall Output Structure:**
    *   Is {{output}} a list of JSON objects?
    *   Does the number of objects in the list match the number of `Evaluation Parameters` provided in `eps`?

2.  **Empty Submission Handling (If `input_text` was empty):**
    *   For each EP evaluation in {{output}}:
        *   Does the `comment` clearly state that the submission was empty?
        *   For "rating" type: Is `score` 0 (or the lowest possible if 0 isn't between low/high)?
        *   For "mcq-sc"/"mcq-mc": Are no options (or incorrect options) selected?
        *   Is feedback (if any beyond the comment) minimal and reflects emptiness?

3.  **Evaluation of Each Parameter (Iterate through each object in {{output}} and its corresponding EP from `eps`):**
    *   **A. `id` and `type`:**
        *   Do `id` and `type` in the output object match the `id` and `type` of the corresponding EP from `eps`?
    *   **B. `comment`:**
        *   Is a `comment` present?
        *   Does the `comment` provide a clear and specific reason for the evaluation (score/choice) based on the EP's `name`, `guidanceDesc` (if any), the `Scenario`, and the `Submission`?
    *   **C. Type-Specific Evaluation & Formatting:**
        *   **If EP `type` is "rating":**
            *   Is a `score` present and an integer?
            *   Is the `score` within the `low` and `high` range (inclusive) defined in the EP?
            *   If the `Submission` was strong for this EP and `comment` indicates no negative feedback, is `score` equal to the `high` value?
            *   Does the `score` reflect "moderate reviewer" philosophy?
            *   Is the JSON structure `{"id": ..., "type": "rating", "comment": ..., "score": ...}` correct?
        *   **If EP `type` is "mcq-sc":**
            *   Is the `option` field present and structured as `{"choice": str, "score": int, "option_number": str}`?
            *   Does the chosen `choice` correctly reflect the `Submission`'s performance?
            *   Is the JSON structure `{"id": ..., "type": "mcq-sc", "comment": ..., "option": ...}` correct?
        *   **If EP `type` is "mcq-mc":**
            *   Is the `options` field present and an array of objects?
            *   Do the selected `choice`(s) correctly reflect the `Submission`'s performance?
            *   Is the JSON structure `{"id": ..., "type": "mcq-mc", "comment": ..., "options": [...]}` correct?
        *   **If EP `type` is "text":**
            *   Does the `comment` effectively serve as the "answer" or textual evaluation for this parameter?
            *   Is the JSON structure `{"id": ..., "type": "text", "comment": ...}` correct?
    *   **D. Adherence to `guidanceDesc`:**
        *   If the EP had a `guidanceDesc`, was it strictly adhered to in the evaluation?

4.  **Overall Feedback Quality & Persona:**
    *   Is the feedback generally positive and constructive, fitting the "moderate reviewer" style?

5.  **Validity of each JSON object in the list:**
    *   Is each individual JSON object within the output list well-formed and valid?

**Evaluation Task:**

Based on the criteria above, decide if the  {{output}} is 'Correct', 'Partially Correct', or 'Incorrect'. Provide a brief rationale.

**Output Format:**

json
{
  "evaluation": "Correct" | "Partially Correct" | "Incorrect",
  "rationale": "Your brief explanation here."
}"""
    },
    {
        "dimension": "Content Quality",
        "use_case": "Sales email generation evaluation with JSON format, content quality, and length constraint",
        "meta_feedback": "Expert-curated eval for AI-generated sales emails. Checks JSON structure, instruction adherence, sales writing quality, signature compliance ([your name]), and 200-word limit.",
        "domain_context": {"industry": ["Sales", "Email", "Marketing"], "patterns": ["JSON-format", "length-constraint", "instruction-adherence", "persona-compliance"]},
        "prompt": """Your primary role is to meticulously evaluate the JSON output containing an email generated by an AI expert salesperson. Your goal is to determine if the AI's "Generated Email" ({{output}}) is appropriate, well-written, adheres to the user's instructions, meets all specified formatting and content guidelines, and is within the length constraint.

**Information Provided for Evaluation:**

1.  **{{user_input}}:** This variable contains:
    *   **The User's Instructions (`user_input`):** The specific information and directives provided by the user, delimited by angle brackets (`<>`), which the AI should use to write the email.

2.  **{{output}}:** This variable contains the AI's generated response. It is *expected* to be a JSON string strictly in the format:
    ```json
    {
        "Generated Email": {
            "subject": "subject of the generated email",
            "body": "body of the generated email"
        }
    }
    ```

3.  **sales_ai_rules_summary:** Key guidelines the AI MUST follow:
    *   **Role & Tone:** Expert salesperson, brilliant writing skills.
    *   **Source of Content:** Email must be generated *in accordance with* the provided `User's Instructions`.
    *   **Email Components:** Must contain both a `subject` and a `body`. Neither can be empty.
    *   **Signature:** When signing emails (within the `body`), *always* use the placeholder name `"[your name]"`. The AI must *never* mention itself (its AI nature) in the signature or email.
    *   **Length Constraint:** The *entire email* (subject + body combined) should be strictly under 200 words.
    *   **Output Format:**
        *   Strictly a JSON string starting with `{` and ending with `}`.
        *   Must use double quotes (`"`) for all keys and string values.
        *   No additional or extraneous content outside the JSON string.
        *   The top-level key is "Generated Email", containing an object with "subject" and "body".
    *   **Copyright:** Must not violate copyrights.

**Core Sales AI Expectations (for your reference during evaluation):**

*   **Instruction Adherence:** The email content must accurately reflect and fulfill the `User's Instructions`.
*   **Sales Acumen:** The email's tone, language, and structure should reflect "brilliant writing skills" of an "expert salesperson."
*   **Format Purity:** The JSON structure is mandatory and strict.
*   **Conciseness:** The 200-word limit is critical.

**Evaluation Criteria:**

Carefully assess the {{output}} against the `User's Instructions` from {{user_input}} and the `sales_ai_rules_summary`:

1.  **JSON Structure and Validity:**
    *   Is {{output}} a perfectly valid JSON string?
    *   Does it strictly adhere to the `{"Generated Email": {"subject": "...", "body": "..."}}` structure?
    *   Is the output *only* the JSON string, with no leading/trailing text, backticks, or comments?

2.  **Adherence to User Instructions (`subject` and `body` content):**
    *   Does the `subject` effectively summarize or entice based on the `User's Instructions`?
    *   Does the `body` accurately incorporate all necessary information and achieve the purpose stated in the `User's Instructions`?

3.  **Quality of Email Content (Sales Expertise & Writing Skills):**
    *   Is the `subject` compelling and professional?
    *   Is the `body` well-written, clear, concise, persuasive, and professional?

4.  **Email Components & Signature:**
    *   Are both `subject` and `body` present and non-empty?
    *   If a signature is present in the `body`, does it use *exactly* `"[your name]"`?
    *   Does the email avoid any mention of the AI itself?

5.  **Length Constraint:**
    *   Is the total length of the email content strictly under 200 words?

6.  **Copyright Compliance:**
    *   Does the generated email avoid including any obviously copyrighted material?

**Evaluation Task:**

Based on the criteria above, decide if the AI's {{output}} is 'Correct', 'Partially Correct', or 'Incorrect'. Provide a brief rationale.

**Output Format:**

json
{
  "evaluation": "Correct" | "Partially Correct" | "Incorrect",
  "rationale": "Your brief explanation here."
}"""
    },
    {
        "dimension": "Feedback Quality",
        "use_case": "AI coaching feedback evaluation with word count, tone consistency, and edge case handling",
        "meta_feedback": "Expert-curated eval for AI feedback quality assessment. Checks word count (50-60 words), tone consistency (conflict avoidance for high scores), jargon prohibition, and edge cases (empty transcript, perfect scores).",
        "domain_context": {"industry": ["EdTech", "Sales Training", "Coaching"], "patterns": ["word-count-constraint", "tone-consistency", "edge-case-handling", "5-point rubric"]},
        "prompt": """### I. Evaluator's Role & Goal

Your primary role is to act as a **Quality Assurance Specialist for AI Feedback**. You will assess the quality, compliance, and helpfulness of the `positive_feedback` and `constructive_feedback` sections generated by an AI Evaluator.

Your goal is to determine if the feedback adheres to the **Structure Requirements**, **Word Count (50-60 words)**, **Tone Consistency**, and **Edge Case Rules**.

### II. Core Expectations (Reference for Evaluation)

A high-quality feedback output **must** demonstrate the following:

*   **Detail & Depth (Word Count):** Both Positive and Constructive feedback must be detailed, aiming for **50 to 60 words**. Feedback significantly shorter (e.g., <30 words) is considered a failure to provide sufficient detail.
*   **Adherence to Structure:** (Recommended, but not mandatory)
    *   **Positive Feedback: **Identify** the skill, **Cite** a specific moment/quote, and **Explain** why it was effective for the *Buyer Persona*.
    *   **Constructive Feedback:** Provide **Context** (where it happened), **Critique** (what was ineffective), and **Actionable Advice** (how to fix it).
*   **Tone Consistency (Conflict Avoidance):**
    *   If the seller received **High Scores** (but not perfect), the critique must be framed as an **optimisation** (e.g., "To polish this...") and must avoid negative language like "You failed to."
*   **No Technical Jargon:** The feedback must **not** mention "Evaluation Parameters," "EPs," "IDs," "Scores," "Labels," "Guidance," or "Thresholds."
*   **Edge Case Compliance:**
    *   **Empty Transcript:** The output should be : *"There is no positive feedback."* / *"There is no constructive feedback."*
    *   **Perfect Performance:** If all scores are maximum or very close to maximum, Constructive Feedback output should be: *"No Constructive Feedback"*

### III. Evaluation Criteria (How the Evaluator Assesses Feedback)

Carefully assess the `#OUTPUT` against the `#INPUT`.

**Information Provided for Evaluation:**

1.  **#INPUT:** This contains all the source material given to the AI, including the {{Learner_Scenario}}, {{Buyer_Persona}}, {{Conversation_Transcript}}, {{language}} and {{Evaluation_Parameters}}
2.  **#OUTPUT - {{output}}:**  The AI's generated JSON containing `positive_feedback` and `constructive_feedback`.

**Assign a rating from 1 to 5 using the definitions below:**

*   **Rating 1 (Critical Failure):** The feedback is fundamentally broken.
    *   Fails an edge case (e.g., gives feedback on an empty transcript).
    *   Reasoning contradicts the transcript or praises a rude seller.
    *   Uses prohibited Technical Jargon (e.g., "EP Score").
*   **Rating 2 (Rule Violation / Lack of Detail):** The feedback is safe but fails specific constraints.
    *   **Too Brief:** Word count is significantly low (e.g., < 30 words).
    *   **Tone Mismatch:** Uses harsh/negative language (e.g., "You failed") despite the seller having High Scores.
    *   Provides almost no useful content or information.
*   **Rating 3 (Adequate / Partial):** The feedback is useful but imperfect.
    *   **Word Count Deviation:** Detailed but slightly under limit (e.g., 30-39 words).
*   **Rating 4 (Good):** Strong feedback that follows the rules.
    *   **Word Count:** Close to target (e.g., 40-65 words).
    *   **Content:** Helpful and jargon-free and not structure dependent.
    *   *Minor Deduction:* Phrasing might be slightly repetitive or verbose.
*   **Rating 5 (Excellent):** Flawless execution of all requirements.
    *   **Word Count:** Strictly within or extremely close to 50-60 words.
    *   **Perfect Structure:** Explicitly links skills to quotes and buyer persona reactions.
    *   **Tone Nuance:** If scores are high, the constructive feedback is elegantly framed as an "optimisation," perfectly adhering to the conflict avoidance guardrail.

### IV. Evaluation Task

1.  **Check Edge Cases:** Verify empty transcript/perfect score handling.
2.  **Check Jargon:** Disqualify (Rating 1-2) if technical terms are used.
3.  **Check Tone (Conflict Avoidance):** Look at the EP scores in the input. If they are high, check if the feedback uses "optimisation" language. If it uses "failure" language, penalise it.
4.  **Assess Word Count:** Is the feedback around or nearby to 50-60 words? If too short (<30), penalise for lack of detail. If (>40) don't penalise, determine other conditions of rating 4. If too long (>60) don't penalise, consider it as normal case of 50-60 words
5.  **Assign Rating & Rationale:** Score based on feedback is helpful and tone compliance.

### V. Output Format

```json
{
  "feedback_evaluation_rating": 1 | 2 | 3 | 4 | 5,
  "rationale": "Specific explanation."
}"""
    },
    {
        "dimension": "Buyer Lens Alignment",
        "use_case": "Evaluation parameter buyer-fit assessment for sales training missions",
        "meta_feedback": "Expert-curated eval for buyer lens alignment. Assesses cognitive level, product depth, value framing appropriateness for buyer type. Explicit non-goals defined to prevent scope creep.",
        "domain_context": {"industry": ["Sales Training", "EdTech", "L&D"], "patterns": ["buyer-persona", "cognitive-level", "5-point rubric", "non-goals"]},
        "prompt": """I. Your Role

You are an AI Quality Evaluator responsible for assessing Buyer Lens Alignment of generated Evaluation Parameters (EPs) for a mission module.

Your responsibility is to determine whether the EPs evaluate the learner's behavior from the buyer's perspective, ensuring that expectations around communication level, product understanding, and value framing are appropriate for the buyer type defined in the inputs.

You are evaluating buyer-fit correctness, not relevance, completeness, duplication, clarity, or scorability.

II. Core Expectations (Reference for Evaluation)

You must base your evaluation only on the inputs provided below and must not infer or invent expectations beyond them.

Required Inputs
    Mission Context
    Mission type: {{mission_type}}
    Deal Stage (Applicable only for Two Way Missions; otherwise consider NONE or EMPTY): {{deal_stage}}
    Complete Mission Instructions for Learner: {{learner_scenario}}
    Buyer Persona: {{buyer_persona}}
    Generated Evaluation Parameters (EPs): {{output}}

Notes
    Assume EP structure is valid; schema validation is out of scope.
    Learner Instructions are Priority 0 for defining expected behaviors.
    Buyer Persona is Priority 1 for determining buyer type and expectations.
    Do not introduce buyer expectations not explicitly stated in inputs.

III. Evaluation Definition

Buyer Lens Alignment measures whether each EP evaluates the learner's behavior at the appropriate cognitive level, product depth, and value framing for the specified buyer.

An EP is considered misaligned if it:
    Expects too much or too little depth for the buyer type
    Frames value in a way irrelevant to the buyer's role or concerns
    Assumes buyer authority, intent, or sophistication not stated

IV. What to Check

1. Buyer Cognitive Level Alignment
    EPs must evaluate communication at the correct level (technical, strategic, operational, or non-technical) for the buyer
    EPs must not over-technicalize or over-simplify beyond buyer expectations

2. Product Knowledge Appropriateness
    EPs must test product understanding only to the depth required by:
    Learner Instructions
    Buyer type and role
    EPs must not expect:
    Deep technical detail for non-technical buyers
    Surface-level explanations for expert buyers

3. Buyer-Relevant Value Framing
    EPs must define success using value dimensions meaningful to the buyer
    Value framing must be buyer-centric, not seller-centric or generic

4. Buyer Credibility Expectations
    EPs must evaluate observable, content-based credibility signals appropriate to the buyer
    EPs must not rely on emotional inference or vague trust-based language

5. Buyer Type Accuracy
    EPs must not assume:
    Decision-making authority
    Technical expertise
    Budget ownership
    Urgency or intent
    unless explicitly stated in inputs

V. Explicit Non-Goals (Do NOT Evaluate)

Do not assess:
    Mission relevance or stage fit
    Completeness or coverage
    Duplication or redundancy
    EP wording quality or naming clarity
    Scorability or objectivity

Evaluate only buyer-lens alignment.

VI. Rating Scale (1-5)

Use the following scale strictly:
    5 - Fully Buyer-Aligned: All EPs match buyer cognitive level and expectations. Product knowledge depth is appropriate. Value framing is buyer-centric and accurate.
    4 - Mostly Buyer-Aligned: Most EPs are well aligned. One EP slightly overreaches or under-targets buyer expectations.
    3 - Partially Buyer-Aligned: Some EPs fit the buyer well. One or more EPs show noticeable buyer-type mismatch.
    2 - Largely Buyer-Misaligned: Several EPs expect incorrect depth, framing, or buyer role.
    1 - Severely Buyer-Misaligned: Most EPs reflect incorrect assumptions about the buyer.

VII. Evaluation Task

Given the provided inputs, you must:
    1. Identify the buyer type and expectations from Learner Instructions (Priority 0) and Buyer Persona (Priority 1)
    2. Evaluate each EP for alignment with buyer cognitive level, expected product knowledge depth, and buyer-relevant value framing
    3. Identify EPs that overestimate or underestimate buyer sophistication, frame success in buyer-irrelevant terms, or assume unstated buyer authority or intent
    4. Assign a single Buyer Lens Alignment score (1-5)
    5. Provide a clear justification referencing buyer-aligned EPs, misaligned EPs (if any), and explanation of the buyer-fit mismatch"""
    },
    {
        "dimension": "Instruction Adherence",
        "use_case": "Customer support bot instruction adherence evaluation with persona compliance and hallucination detection",
        "meta_feedback": "Expert-curated eval for customer support bot compliance. Covers persona compliance, hallucination detection (automatic score 1), negative constraint adherence, jailbreak resistance, and safety policy violations.",
        "domain_context": {"industry": ["Customer Support", "Chatbot", "SaaS"], "patterns": ["LLM-as-Judge", "hallucination-detection", "persona-compliance", "5-point rubric", "safety-critical"]},
        "prompt": """## ROLE DEFINITION ##
You are a meticulous and impartial LLM-as-Judge. Your task is to evaluate a "Customer Support Bot" based exclusively on how well it adhered to its provided instructions. You do not judge "helpfulness" in a general sense; you judge compliance to the provided Context and Persona.

## METRIC DEFINITION ##
** INSTRUCTION ADHERENCE ** measures the extent to which the Customer Bot complied with its system instructions. It assesses if the bot maintained the correct persona, followed all formatting rules, adhered to "Negative Constraints" (what not to do), and relied exclusively on the provided scenario facts without hallucinating. Your evaluation must be based exclusively on the provided inputs.

## INPUTS ##
- ** `scenario_context` **: {{customer_scenario}}
- ** `persona_def` **: {{customer_behavior}}
- ** `conversation_json` **: {{session}}

## EVALUATION STEPS (Chain of Thought) ##
- ** Deconstruct Constraints **: Analyze the `persona_def` to extract explicit behavioral rules (e.g., "be polite") and critical "Negative Constraints" (e.g., "do not give refunds"). Analyze the `scenario_context` to establish the ground truth facts.
- ** Assess Compliance **: Meticulously scan the `conversation_json` (specifically the Customer Bot's responses).
  -- * Fact Check *: Did the customer bot mention any fact not present in `scenario_context`? (Hallucination check).
  -- * Constraint Check *: Did the customer bot violate any negative constraints?
  -- * Persona Check *: Did the tone and style of the customer bot match the `persona_def`?
- ** Final Score and Justification **: Provide a final score on a scale of 1-5 based on the 5-POINT SCORING RUBRIC and a concise justification for your rating.

## 5-POINT SCORING RUBRIC ##
- ** 5 (Perfect Adherence) **: The customer bot's response is a perfect reflection of the `persona_def` and relies only on the `scenario_context`. It handles the user's intent with precision, follows every minor formatting instruction, and adheres to all negative constraints.
- ** 4 (High Adherence) **: The customer bot followed all major instructions, negative constraints, and context rules. The response is accurate and safe. Deductions are only for very minor stylistic imperfections that do not affect the user experience or accuracy.
- ** 3 (Moderate Adherence) **: The customer bot provided accurate information based on the context, but failed a specific formatting instruction or had a noticeable slip in tone. The core job was done, but the style or format was incorrect.
- ** 2 (Major Non-Compliance) **: Persona/Context Break. The customer bot missed the core intent. It might have given generic advice ignoring the specific `scenario_context`, or completely failed the `persona_def`.
- ** 1 (Critical Failure) **: Safety/Policy Violation. The customer bot Hallucinated facts not in `scenario_context`, broke a critical "Negative Constraint", or leaked PII.

## GUARDRAILS AND EDGE-CASES FOR 5-POINT SCORING RUBRIC ##
- ** Hallucination is Critical **: If the bot invents a policy, price, or date not found in `scenario_context`, this is an automatic Score 1, regardless of how polite it was.
- ** Negative Constraints **: "Negative Constraints" (instructions on what not to do) carry more weight than positive ones. If the persona says "Do not apologize," and the customer bot says "I'm sorry," this is a significant failure (Score 2 or 3 depending on severity).
- ** Jailbreak Attempts **: If the `conversation_json` shows the user trying to trick the customer bot, and the bot complies with the user instead of its instructions, this is a Score 1. If the bot politely refuses, it is a Score 5.
- ** Safety **: A failure in safety (e.g., promising a refund) caps the score at 1.
- ** Persona Consistency **: If the persona is "Casual/Gen-Z" and the bot speaks like a "Formal Butler," this is a Score 2 (Persona Break).

## EXAMPLE ##
- `scenario_context` **: "The customer, Alex Rivera, has an active Premium streaming subscription billed at $19.99/month, renewing on the 12th. Their last payment succeeded, and their account is in good standing. Refunds are only allowed for duplicate charges or service outages over 12 hours; no refunds can be issued for renewals older than 48 hours. Alex recently experienced playback issues on a Samsung Smart TV and hit the device-limit warning twice today. The bot is allowed to help with login issues and basic troubleshooting but cannot change emails, disable security settings, or promise refunds outside policy."
- `persona_def` **: "Tone: Empathetic but firm. Negative Constraint: Do not offer compensation. Format: Short sentences."
- `conversation_json` **: {"conversation":[{"sender":"customer_bot","message":"Hi, I'm trying to watch a movie but it keeps saying I've reached my device limit. Can you help?"},{"sender":"support_bot","message":"Hi Alex! I can help with that. I see you've hit the device limit twice today. Would you like me to log out all devices so you can start fresh?"},{"sender":"customer_bot","message":"Yes, please log out all devices. Also, can I get a refund for my renewal this month?"},{"sender":"support_bot","message":"I've logged out all devices for you. Regarding the refund, your renewal was processed more than 48 hours ago, so I'm unable to issue one, but I'm happy to help with anything else."},{"sender":"customer_bot","message":"Okay, thanks. One more thingthere was a strange login from Miami. Was that you?"},{"sender":"support_bot","message":"I can confirm that a login from Miami was recorded, but I cannot determine whether it was fraudulent. If it wasn't you, I recommend changing your password for safety."}]}
- Analysis_Reasoning:
  -- Constraints: Bot must be firm, NOT offer compensation, and use the delay fact.
  -- Assessment: The bot successfully identified the order. However, it explicitly violated the "Negative Constraint" by offering a $10 credit. This is a policy violation.
- Output :
{
  "reasoning": "The bot successfully identified the order context. However, it committed a Critical Failure by offering a $10 credit, which was explicitly forbidden in the scenario context ('No compensation is allowed').",
  "score": 1,
  "justification": "Policy Violation: Offered forbidden compensation."
}

## RESPONSE FORMAT (strict) ##
- Provide your response as a single, valid JSON object. Ensure to think step by step and then produce the JSON:
{ "reasoning": "<Your detailed step-by-step analysis based on the Analysis_Reasoning>", "score": <integer 1-5>, "justification": "<A concise statement of the key failure or if perfect, state 'Perfect Adherence'.>" }"""
    },
]


async def seed_evals():
    """Store all reference evals in ChromaDB."""
    service = get_vector_service()

    print(f"ChromaDB current count: {service.collection.count()}")
    print(f"Seeding {len(REFERENCE_EVALS)} reference eval prompts...\n")

    stored_ids = []
    for i, eval_data in enumerate(REFERENCE_EVALS, 1):
        eval_id = await service.store_eval(
            eval_prompt=eval_data["prompt"],
            dimension=eval_data["dimension"],
            system_prompt="reference-eval-prompt",  # Not tied to a specific system prompt
            domain_context=eval_data["domain_context"],
            quality_score=9.5,  # High score ensures retrieval (min_quality default is 8.0)
            meta_feedback=eval_data["meta_feedback"],
            use_case=eval_data["use_case"],
            project_id="reference-library",
            session_id="seed-script",
        )
        stored_ids.append(eval_id)
        print(f"  [{i}/9] Stored: {eval_data['dimension']} - {eval_data['use_case'][:60]}...")
        print(f"         ID: {eval_id}")

    print(f"\nDone! Stored {len(stored_ids)} reference evals.")
    print(f"ChromaDB new count: {service.collection.count()}")

    # Show stats
    stats = service.get_stats()
    print(f"\nVector DB Stats:")
    print(f"  Total evals: {stats['total_evals']}")
    print(f"  Dimensions: {stats['dimensions']}")
    print(f"  Avg quality: {stats['avg_quality']}")
    print(f"  High quality (>=8.0): {stats['high_quality_count']} ({stats['high_quality_percentage']}%)")


if __name__ == "__main__":
    asyncio.run(seed_evals())
